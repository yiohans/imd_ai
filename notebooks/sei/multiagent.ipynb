{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "wandb_key_preview = os.getenv(\"WANDB_API_KEY\")[:10]\n",
    "print(f\"First 10 characters of W&B key: {wandb_key_preview}\")\n",
    "\n",
    "groq_key_preview = os.getenv(\"GROQ_API_KEY\")[:10]\n",
    "print(f\"First 10 characters of Groq key: {groq_key_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Ollama dependencies\n",
    "\n",
    "1. `pciutils` is required by Ollama to detect the GPU type.\n",
    "2. Installation of Ollama in the runtime instance will be taken care by `curl -fsSL https://ollama.com/install.sh | sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "  !sudo apt update -qq\n",
    "  !sudo apt install -qq -y pciutils\n",
    "  !curl -fsSL https://ollama.com/install.sh | sh\n",
    "else:\n",
    "    print(\"Not running in Google Colab\")\n",
    "    ! if ! ollama --version; then echo \"ollama is not installed\" && exit 1; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Ollama\n",
    "---\n",
    "\n",
    "In order to use Ollama it needs to run as a service in background parallel to your scripts. Because Jupyter Notebooks is built to run code blocks in sequence this make it difficult to run two blocks at the same time. As a workaround we will create a service using subprocess in Python so it doesn't block any cell from running.\n",
    "\n",
    "Service can be started by command `ollama serve`.\n",
    "\n",
    "`time.sleep(5)` adds some delay to get the Ollama service up before downloading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "# Check if ollama is running\n",
    "try:\n",
    "  response = requests.get('http://localhost:11434')\n",
    "  if response.status_code == 200:\n",
    "    print(\"Ollama is running\")\n",
    "except:\n",
    "  print(\"Ollama is not running\")\n",
    "  thread = threading.Thread(target=run_ollama_serve)\n",
    "  thread.start()\n",
    "  time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "# from langchain_experimental.utilities.python import PythonREPL\n",
    "\n",
    "# repl = PythonREPL()\n",
    "\n",
    "# @tool\n",
    "# def python_repl_tool(\n",
    "#     code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "# ):\n",
    "#     \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "#     you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "#     try:\n",
    "#         result = repl.run(code)\n",
    "#     except BaseException as e:\n",
    "#         return f\"Failed to execute. Error: {repr(e)}\"\n",
    "#     result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "#     return (\n",
    "#         result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "#     )\n",
    "\n",
    "@tool\n",
    "def search_process(id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for a SEI process folder in the file system.\n",
    "\n",
    "    Use this function to locate administrative process documents by their reference number.\n",
    "    The function handles both traditional (166/2025) and compact (1662025) ID formats.\n",
    "\n",
    "    Args:\n",
    "        id (str): Process number in either format:\n",
    "            - Separated format: \"166/2025\"\n",
    "            - Compact format: \"1662025\"\n",
    "            The number will be automatically padded if needed.\n",
    "\n",
    "    Returns:\n",
    "        str: One of:\n",
    "            - Folder name (e.g., \"SEI_00166_2025\") if process exists\n",
    "            - None if process not found (compact format)\n",
    "            - \"Process not found\" if process not found (separated format or errors)\n",
    "\n",
    "    Example:\n",
    "        To find process 166/2025:\n",
    "        > search_process(\"166/2025\")\n",
    "        Returns: \"SEI_00166_2025\"\n",
    "\n",
    "        To find same process with compact format:\n",
    "        > search_process(\"1662025\")\n",
    "        Returns: \"SEI_00166_2025\"\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    root_path = os.path.abspath(\"\")\n",
    "    processes_path = os.path.join(root_path, \"processos\")\n",
    "    try:\n",
    "        if len(id) < 9:\n",
    "            if id.find(\"/\") == -1:\n",
    "                id = id.zfill(9)\n",
    "            else:\n",
    "                id = id.split(\"/\")\n",
    "                id[0] = id[0].zfill(5)\n",
    "                id[1] = id[1]\n",
    "                id = \"/\".join(id)\n",
    "        if id.find(\"/\") == -1:\n",
    "            folder = f\"SEI_{id[:-4]}_{id[-4:]}\"\n",
    "            # print(f\"Searching for {folder}\")\n",
    "            if os.path.exists(os.path.join(processes_path, folder)):\n",
    "                # print(f\"Process {id} found!\")\n",
    "                return folder\n",
    "            else:\n",
    "                # print(f\"Process {id} not found!\")\n",
    "                return \"Process not found\"\n",
    "        else:\n",
    "            folder = f\"SEI_{id.split('/')[0]}_{id.split('/')[1]}\"\n",
    "            # print(f\"Searching for {folder}\")\n",
    "            if os.path.exists(os.path.join(processes_path, folder)):\n",
    "                # print(f\"Process {id} found!\")\n",
    "                return folder\n",
    "            else:\n",
    "                # print(f\"Process {id} not found!\")\n",
    "                return \"Process not found\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Process not found\"\n",
    "\n",
    "@tool\n",
    "def get_documents_from_process(\n",
    "    parameters: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve PDF documents from a SEI process folder with pagination support.\n",
    "\n",
    "    Use this function to get a list of PDF documents within a process folder.\n",
    "    Results can be paginated using limit and offset parameters.\n",
    "    Typically used after locating a process folder with search_process().\n",
    "\n",
    "    Args:\n",
    "        parameters (str): A string containing the process folder name and pagination parameters.\n",
    "            The string should be formatted as follows:\n",
    "            \"process_folder,limit,offset\"\n",
    "            - process_folder: The name of the process folder (e.g., \"SEI_00166_2025\")\n",
    "            - limit: The maximum number of documents to return (default: 10)\n",
    "            - offset: The number of documents to skip (default: 0)\n",
    "\n",
    "    Returns:\n",
    "        Union[list[str], str]: One of:\n",
    "            - List of PDF filenames if documents are found\n",
    "            - \"Invalid parameters\" if parameters are incorrect\n",
    "            - \"Process folder not found\" if folder doesn't exist or error occurs\n",
    "\n",
    "    Example:\n",
    "        # Get first 10 documents\n",
    "        > get_documents_from_process(\"SEI_00166_2025\")\n",
    "        Returns: [\"doc1.pdf\", \"doc2.pdf\", ...]\n",
    "\n",
    "        # Get next 10 documents\n",
    "        > get_documents_from_process(\"SEI_00166_2025\", limit=10, offset=10)\n",
    "        Returns: [\"doc11.pdf\", \"doc12.pdf\", ...]\n",
    "\n",
    "    Note:\n",
    "        - Only returns PDF files\n",
    "        - Use with search_process() to locate folder first\n",
    "        - Empty list means no documents found in range\n",
    "    \"\"\"\n",
    "    import os\n",
    "    try:\n",
    "        process_folder, limit, offset = parameters.split(\",\")\n",
    "        limit = int(limit)\n",
    "        offset = int(offset)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Invalid parameters\"\n",
    "    try:\n",
    "        tree = os.walk(os.path.join(os.path.abspath(\"\"), \"processos\", process_folder))\n",
    "        documents = []\n",
    "        for root, dirs, files in tree:\n",
    "            for file in files:\n",
    "                documents.extend([\n",
    "                    file\n",
    "                    for file in files\n",
    "                    if file.endswith(\".pdf\")\n",
    "                    ])\n",
    "        return documents[offset:offset+limit]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Process folder not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = search_process(\"166/2025\")\n",
    "\n",
    "get_documents_from_process(f\"{folder},5,10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" make sure to prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the chat model\n",
    "model_name = 'llama3.1'  # Change this to the model you want to use\n",
    "\n",
    "# Download the model from the Ollama server\n",
    "!ollama pull $model_name\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model_name,  # Specify the model version\n",
    "    base_url=\"http://localhost:11434\",  # URL where Ollama is running locally\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        return END\n",
    "    return goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[search_process, get_documents_from_process],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only search for SEI processes and get documents from them.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"researcher\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # # AI message at the last position of the input messages list\n",
    "    # result[\"messages\"][-1] = HumanMessage(\n",
    "    #     content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    # )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "    \n",
    "# # Chart generator agent and node\n",
    "# # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "# chart_agent = create_react_agent(\n",
    "#     llm,\n",
    "#     [python_repl_tool],\n",
    "#     prompt=make_system_prompt(\n",
    "#         \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# def chart_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "#     result = chart_agent.invoke(state)\n",
    "#     goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "#     # wrap in a human message, as not all providers allow\n",
    "#     # AI message at the last position of the input messages list\n",
    "#     result[\"messages\"][-1] = HumanMessage(\n",
    "#         content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "#     )\n",
    "#     return Command(\n",
    "#         update={\n",
    "#             # share internal message history of chart agent with other agents\n",
    "#             \"messages\": result[\"messages\"],\n",
    "#         },\n",
    "#         goto=goto,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "# workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle\n",
    "\n",
    "try:\n",
    "    # Generate and display a visual representation of the workflow graph\n",
    "    # The get_graph method is called on the app object with xray=True to include detailed information\n",
    "    # The draw_mermaid_png method converts the graph to a PNG image using Mermaid.js\n",
    "    # The curve_style parameter is set to CurveStyle.NATURAL to use smooth curves for the edges in the graph\n",
    "    graph_image = graph.get_graph(xray=True).draw_mermaid_png(\n",
    "        curve_style=CurveStyle.NATURAL\n",
    "    )\n",
    "\n",
    "    # Display the generated image in the Jupyter notebook\n",
    "    display(Image(graph_image))\n",
    "except Exception as e:\n",
    "    # If an error occurs during the graph generation or display, print the error message\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Is there a process with the number 166/2025? If yes, please list the files 11 to 15.\",\n",
    "            )\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(inputs, stream_mode=\"values\")\n",
    "\n",
    "print_stream(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = graph.invoke(inputs)\n",
    "\n",
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
