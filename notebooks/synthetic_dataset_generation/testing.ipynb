{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkKTJOgHDqkU"
      },
      "source": [
        "## Installing Ollama dependencies\n",
        "---\n",
        "\n",
        "1. `pciutils` is required by Ollama to detect the GPU type.\n",
        "2. Installation of Ollama in the runtime instance will be taken care by `curl -fsSL https://ollama.com/install.sh | sh`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlVK9iG4AD5L",
        "outputId": "36c17e8d-8105-4577-f9ef-eda76ac3fc7f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "  !sudo apt update -qq\n",
        "  !sudo apt install -qq -y pciutils\n",
        "  !curl -fsSL https://ollama.com/install.sh | sh\n",
        "else:\n",
        "    print(\"Not running in Google Colab\")\n",
        "    ! if ! ollama --version; then echo \"ollama is not installed\"; exit(1); fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbezG5uADxBB"
      },
      "source": [
        "## Running Ollama\n",
        "---\n",
        "\n",
        "In order to use Ollama it needs to run as a service in background parallel to your scripts. Because Jupyter Notebooks is built to run code blocks in sequence this make it difficult to run two blocks at the same time. As a workaround we will create a service using subprocess in Python so it doesn't block any cell from running.\n",
        "\n",
        "Service can be started by command `ollama serve`.\n",
        "\n",
        "`time.sleep(5)` adds some delay to get the Ollama service up before downloading the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA3Cme2oDVbe"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "  \n",
        "# Check if ollama is running\n",
        "try:\n",
        "  response = requests.get('http://localhost:11434')\n",
        "  if response.status_code == 200:\n",
        "    print(\"Ollama is running\")\n",
        "except:\n",
        "  print(\"Ollama is not running\")\n",
        "  thread = threading.Thread(target=run_ollama_serve)\n",
        "  thread.start()\n",
        "  time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxSFHl9D4H4"
      },
      "source": [
        "## Runing project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vW6HBzf6F1K",
        "outputId": "e9172537-04df-48d9-c04f-7d32ecb1784b"
      },
      "outputs": [],
      "source": [
        "%pip install dotenv weave langchain_core langchain_openai langchain_ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu9WX26p591c",
        "outputId": "be8e17d6-67d8-4270-8d12-30df1f3fac2f"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key_preview = os.getenv(\"OPENAI_API_KEY\")[:10]\n",
        "print(f\"First 10 characters of API key: {api_key_preview}\")\n",
        "\n",
        "wandb_key_preview = os.getenv(\"WANDB_API_KEY\")[:10]\n",
        "print(f\"First 10 characters of W&B key: {wandb_key_preview}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1vIKcPn591d"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-tgIdfC591d",
        "outputId": "db9fb4ab-cfdc-4d25-b6de-170e0ea60c5d"
      },
      "outputs": [],
      "source": [
        "weave.init(\"langchain_demo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXdDGI1n591e"
      },
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# llm = ChatOpenAI()\n",
        "# prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
        "\n",
        "# llm_chain = prompt | llm\n",
        "\n",
        "# output = llm_chain.invoke({\"number\": 2})\n",
        "\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dazepnWV591e",
        "outputId": "0a571b4b-33bb-4489-ccfd-76f264b17a91"
      },
      "outputs": [],
      "source": [
        "model = 'llama3'\n",
        "!ollama pull $model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDRwq7Gh591e"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama.chat_models import ChatOllama\n",
        "\n",
        "# Initialize the ChatOllama model\n",
        "model_llama = ChatOllama(\n",
        "    model=model,  # Specify the model version\n",
        "    base_url=\"http://localhost:11434\",  # URL where Ollama is running locally\n",
        "    temperature=0.7,  # Control the randomness of the output (0.0 to 1.0)\n",
        ")\n",
        "\n",
        "# Note: Ensure Ollama is running on your computer before executing this code\n",
        "\n",
        "# If you encounter an OllamaEndpointNotFoundError, you may need to pull the model\n",
        "# Run the following command in your terminal:\n",
        "# ollama pull llama3.1\n",
        "\n",
        "# Generate a response from the model\n",
        "response = model_llama.invoke(\"Olá, meu nome é Yuri. Qual é o seu nome?\")\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
