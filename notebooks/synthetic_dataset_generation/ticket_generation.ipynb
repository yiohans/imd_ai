{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWo-PM5A-u1q"
      },
      "source": [
        "## Prepare environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHUZZkJe-u1t"
      },
      "source": [
        "### Install project dependencies\n",
        "\n",
        "Install the remaining Python dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vW6HBzf6F1K",
        "outputId": "1240c2ee-7e4b-42d3-c024-a8e89c1b0974"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "project_dependencies = \"dotenv weave langchain_core langchain_openai langchain_ollama langchain-google-genai langchain-groq\"\n",
        "\n",
        "# Try to install using poetry first and then pip\n",
        "try:\n",
        "    response = subprocess.check_output([\"poetry\", \"--version\"])\n",
        "    print(\"Poetry is installed\")\n",
        "    !poetry add $project_dependencies\n",
        "except:\n",
        "    print(\"Poetry is not installed. Using pip to install dependencies\")\n",
        "    %pip install -qU $project_dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtaVvcLr-u1u"
      },
      "source": [
        "### Load API Keys from environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu9WX26p591c",
        "outputId": "348d6ba7-e66b-4667-d499-0b935dc41e0d"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key_preview = os.getenv(\"OPENAI_API_KEY\")[:10]\n",
        "print(f\"First 10 characters of API key: {api_key_preview}\")\n",
        "\n",
        "wandb_key_preview = os.getenv(\"WANDB_API_KEY\")[:10]\n",
        "print(f\"First 10 characters of W&B key: {wandb_key_preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxSFHl9D4H4"
      },
      "source": [
        "## Run project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s4H3kdU-u1u"
      },
      "source": [
        "### Initialize tracking with Weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1vIKcPn591d",
        "outputId": "11b90c63-86ef-4b7e-c850-a16f0a852b15"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "weave.init(\"synthetic_ticket_dataset_generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1TQyqE5-u1v"
      },
      "source": [
        "### Choose a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5vbwZMP-u1v"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize the chat model\n",
        "chat_model = ChatGroq(\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        "    temperature=0.6,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4BYR1qB-u1v"
      },
      "outputs": [],
      "source": [
        "def parse_response(prompt, response):\n",
        "    content = response.content\n",
        "    ## Remove everything between <think> and </think>\n",
        "    think_start = content.find(\"<think>\")\n",
        "    think_end = content.find(\"</think>\")\n",
        "    thought = content[think_start + len(\"<think>\"):think_end]\n",
        "    final_answer = content[think_end + len(\"</think>\"):]\n",
        "    parsed_response = {\n",
        "        \"prompt\" : str(prompt).strip(),\n",
        "        \"final_answer\": final_answer.strip(),\n",
        "        \"thought\" : thought.strip()\n",
        "    }\n",
        "    return parsed_response\n",
        "\n",
        "def parse_responses(prompts, responses):\n",
        "    parsed_responses = []\n",
        "    for prompt, response in zip(prompts, responses):\n",
        "        parsed_response = parse_response(prompt, response)\n",
        "        parsed_responses.append(parsed_response)\n",
        "    return parsed_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1PszvYE-u1v"
      },
      "outputs": [],
      "source": [
        "tipos_chamados = {\n",
        "    '1' : \"Equipamentos de Informática: Solicitações relacionadas a hardware, como computadores, impressoras e periféricos.\",\n",
        "    '2' : \"Acesso a Sistemas: Solicitações referentes a problemas de login, senha, permissões ou acesso a plataformas.\",\n",
        "    '3' : \"Comunicação e Redes: Solicitações envolvendo infraestrutura de rede, internet, telefonia e outros serviços de comunicação.\",\n",
        "    '4' : \"Outros: Problemas que não se enquadram nas categorias acima.\"\n",
        "}\n",
        "\n",
        "tipos_chamados_for_prompt = \" \\n\".join([f\"{k}: {v}\" for k, v in tipos_chamados.items()])\n",
        "print(tipos_chamados_for_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1OUnM1U-u1v"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_for_classification = ChatPromptTemplate([\n",
        "    (\"system\",\n",
        "     f\"\"\"Você é um assistente de IA especializado em classificação de chamados de suporte técnico.\n",
        "    Classifique o chamado em um dos tipos seguintes conforme a descrição do que engloba cada chamado.\n",
        "    A resposta deve ser apenas o número do tipo do chamado.\n",
        "    {tipos_chamados_for_prompt}\"\"\"\n",
        "    ),\n",
        "    (\"user\", \"{chamado}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JEl-Dr--u1w",
        "outputId": "208b03b1-8b82-4764-9b6e-41f72a913216"
      },
      "outputs": [],
      "source": [
        "chamados = [\n",
        "    \"O computador não liga.\",\n",
        "    # \"Não consigo acessar o sistema.\",\n",
        "    # \"A internet está lenta.\",\n",
        "    # \"O telefone não funciona.\"\n",
        "]\n",
        "\n",
        "responses = []\n",
        "for chamado in chamados:\n",
        "    response = chat_model.invoke(prompt_for_classification.invoke({\"chamado\": chamado}))\n",
        "    responses.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssfdRHKR-u1w"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"classification.json\", \"w\") as f:\n",
        "    answers = parse_responses(chamados, responses)\n",
        "    json.dump(answers, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variedade_problemas = [\n",
        "    \"Ambiente Corporativo: Problema ocorrido em um ambiente de escritório com múltiplos usuários.\",\n",
        "    \"Home Office: Questões relatadas por usuários que trabalham remotamente.\",\n",
        "    \"Infraestrutura Crítica: Problemas que afetam sistemas essenciais ou serviços críticos.\",\n",
        "    \"Ocorrência Intermitente: Problemas que surgem de forma esporádica e sem padrão definido.\",\n",
        "    \"Impacto Regional: Problemas que afetam não apenas um usuário, mas uma área ou setor específico.\",\n",
        "    \"Complexidade Elevada: Problemas com múltiplas causas ou que exigem uma solução abrangente.\"\n",
        "]\n",
        "\n",
        "componentes_afetados = {\n",
        "    \"1\": [\n",
        "        \"Hardware\",\n",
        "        \"Sistema Operacional\",\n",
        "        \"Outros\"\n",
        "    ],\n",
        "    \"2\": [\n",
        "        \"Aplicativo Corporativo\",\n",
        "        \"Segurança da Informação\",\n",
        "        \"Base de Dados\",\n",
        "        \"Outros\"\n",
        "    ],\n",
        "    \"3\": [\n",
        "        \"Rede e Conectividade\",\n",
        "        \"Segurança da Informação\",\n",
        "        \"Outros\"\n",
        "    ],\n",
        "    \"4\": [\n",
        "        \"Outros\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-VSW3Ei-u1w"
      },
      "outputs": [],
      "source": [
        "prompt_for_generation = ChatPromptTemplate([\n",
        "    (\"system\",\n",
        "     \"\"\"Você é um assistente de IA especialista em suporte técnico e na classificação de chamados. \n",
        "Seu objetivo é simular um exemplo realista de chamado de suporte técnico, utilizando as informações fornecidas.\n",
        "\n",
        "Tipos de Chamados Disponíveis:\n",
        "{tipos_chamados_for_prompt}\n",
        "\n",
        "Instruções:\n",
        "1. O chamado deve corresponder ao tipo indicado pelo usuário.\n",
        "2. A situação deve envolver o componente mencionado pelo usuário.\n",
        "3. O chamado deve conter a especifidade do problema\n",
        "3. Gere um exemplo coerente e realista contendo apenas um título e uma descrição.\n",
        "4. Formate a resposta exatamente como:\n",
        "   \"<Título>\"; \"<Descrição>\"\n",
        "5. Não adicione informações extras, comentários ou conteúdo além do solicitado.\n",
        "\n",
        "Exemplo:\n",
        "\"Erro ao acessar o sistema de RH\"; \"Ao tentar acessar o sistema de RH, recebi uma mensagem de erro que indica uma falha no aplicativo corporativo.\"\n",
        "\n",
        "Produza um único exemplo de chamado.\n",
        "\"\"\"\n",
        "    ),\n",
        "    (\"user\", \"Tipo: {tipo}\\nComponente Afetado: {componente_afetado}\\nEspecifidade do Problema:{detalhe_problema}\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "ZUIBnz56-u1w",
        "outputId": "428e78b5-5868-43a0-a24a-5dee0eb99e4a"
      },
      "outputs": [],
      "source": [
        "import random, time\n",
        "\n",
        "number_of_tickets_required = 5000\n",
        "\n",
        "try:\n",
        "    with open(\"generation.json\", \"r\") as f:\n",
        "      ticket_analysis_results = json.loads(f.read())\n",
        "      progress = len(ticket_analysis_results) / number_of_tickets_required * 100\n",
        "      print(f\"Resuming from saved progress: {progress:.2f}%\")\n",
        "except:\n",
        "    ticket_analysis_results = []\n",
        "\n",
        "number_of_tickets_generated = len(ticket_analysis_results)\n",
        "\n",
        "tipos_solicitados = random.choices(list(tipos_chamados.keys()), k= number_of_tickets_required-number_of_tickets_generated)\n",
        "number_tries_rate_limit = 0\n",
        "\n",
        "for i in range(len(tipos_solicitados)):\n",
        "    tipo = tipos_solicitados[i]\n",
        "    try:\n",
        "        response = chat_model.invoke(\n",
        "            prompt_for_generation.invoke({\n",
        "              \"tipos_chamados_for_prompt\": tipos_chamados_for_prompt,\n",
        "              \"tipo\": tipo,\n",
        "              \"componente_afetado\": random.choice(list(componentes_afetados[f'{tipo}'])),\n",
        "              \"detalhe_problema\": random.choice(variedade_problemas)}))\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        # print(f\"Error: {error_message}\")\n",
        "        if \"429\" in error_message:\n",
        "            number_tries_rate_limit += 1\n",
        "            print('\\r\\x1b[2K',\n",
        "                  f\"\\rProgress: {progress:.2f}%\",\n",
        "                  f\" - Rate limit exceeded. Waiting for {number_tries_rate_limit} minutes.\", end=\"\")\n",
        "            time.sleep(60)\n",
        "            i -= 1\n",
        "    else:\n",
        "        try:\n",
        "            with open(\"generation.json\", \"r\") as f:\n",
        "                saved_answers = json.loads(f.read())\n",
        "        except:\n",
        "            saved_answers = []\n",
        "        with open(\"generation.json\", \"w\") as f:\n",
        "            answers = parse_response(tipo, response)\n",
        "            saved_answers.append(answers)\n",
        "            json.dump(saved_answers, f, indent=2)\n",
        "        time.sleep(3)\n",
        "        number_tries_rate_limit = 0\n",
        "        progress = len(saved_answers) / number_of_tickets_required * 100\n",
        "        print('\\r\\x1b[2K', f\"\\rProgress: {progress:.2f}%\", end=\"\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check generated dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Ollama dependencies\n",
        "\n",
        "1. `pciutils` is required by Ollama to detect the GPU type.\n",
        "2. Installation of Ollama in the runtime instance will be taken care by `curl -fsSL https://ollama.com/install.sh | sh`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# IN_COLAB = 'google.colab' in sys.modules\n",
        "# if IN_COLAB:\n",
        "#   !sudo apt update -qq\n",
        "#   !sudo apt install -qq -y pciutils\n",
        "#   !curl -fsSL https://ollama.com/install.sh | sh\n",
        "# else:\n",
        "#     print(\"Not running in Google Colab\")\n",
        "#     ! if ! ollama --version; then echo \"ollama is not installed\" && exit 1; fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Starting Ollama\n",
        "---\n",
        "\n",
        "In order to use Ollama it needs to run as a service in background parallel to your scripts. Because Jupyter Notebooks is built to run code blocks in sequence this make it difficult to run two blocks at the same time. As a workaround we will create a service using subprocess in Python so it doesn't block any cell from running.\n",
        "\n",
        "Service can be started by command `ollama serve`.\n",
        "\n",
        "`time.sleep(5)` adds some delay to get the Ollama service up before downloading the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA3Cme2oDVbe",
        "outputId": "277a8004-b734-4557-f5a1-53407e8f2be8"
      },
      "outputs": [],
      "source": [
        "# import threading\n",
        "# import subprocess\n",
        "# import time\n",
        "# import requests\n",
        "\n",
        "# def run_ollama_serve():\n",
        "#   subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# # Check if ollama is running\n",
        "# try:\n",
        "#   response = requests.get('http://localhost:11434')\n",
        "#   if response.status_code == 200:\n",
        "#     print(\"Ollama is running\")\n",
        "# except:\n",
        "#   print(\"Ollama is not running\")\n",
        "#   thread = threading.Thread(target=run_ollama_serve)\n",
        "#   thread.start()\n",
        "#   time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choose model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Uncomment the following code to change to Ollama\n",
        "\n",
        "# from langchain_ollama import ChatOllama\n",
        "\n",
        "# # Initialize the chat model\n",
        "# model = 'deepseek-r1:14b'\n",
        "# !ollama pull $model\n",
        "# chat_model = ChatOllama(\n",
        "#     model=model,  # Specify the model version\n",
        "#     base_url=\"http://localhost:11434\",  # URL where Ollama is running locally\n",
        "#     temperature=0.6,  # Control the randomness of the output (0.0 to 1.0)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"generation.json\", \"r\") as f:\n",
        "    answers = json.loads(f.read())\n",
        "    print(f\"Total tickets generated: {len(answers)}\")\n",
        "    tickets_sample = [{'final_answer': ticket['final_answer'], 'prompt' : ticket['prompt']} for ticket in answers]\n",
        "\n",
        "print(\"Analyzing generated tickets...\")\n",
        "try:\n",
        "    with open(\"generation_analysed.json\", \"r\") as f:\n",
        "        ticket_analysis_results = json.loads(f.read())\n",
        "except:\n",
        "    ticket_analysis_results = []\n",
        "    \n",
        "progress = len(ticket_analysis_results) / len(tickets_sample) * 100\n",
        "print(f\"Resuming from saved progress: {progress:.2f}%\")\n",
        "print(\"Total tickets analyzed:\", len(ticket_analysis_results))\n",
        "print(\"Total tickets to analyze:\", len(tickets_sample))\n",
        "\n",
        "number_tries_rate_limit = 0\n",
        "\n",
        "for i in range(len(ticket_analysis_results), len(tickets_sample)):\n",
        "    tickets = tickets_sample[i]\n",
        "    ticket_text = tickets['final_answer']\n",
        "    ticket_class = tickets['prompt']\n",
        "    try:\n",
        "        response = chat_model.invoke(prompt_for_classification.invoke({\"chamado\": ticket_text}))\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        # print(f\"Error: {error_message}\")\n",
        "        if \"429\" in error_message:\n",
        "            number_tries_rate_limit += 1\n",
        "            print('\\r\\x1b[2K',\n",
        "                f\"\\rProgress: {progress:.2f}%\",\n",
        "                f\" - Rate limit exceeded. Waiting for {number_tries_rate_limit} minutes.\", end=\"\")\n",
        "            time.sleep(60)\n",
        "            i -= 1\n",
        "    else:\n",
        "        with open(\"generation_analysed.json\", \"w\") as f:\n",
        "            answers = parse_response(ticket_text, response)\n",
        "            answers['correct'] = (answers['final_answer'] == ticket_class)\n",
        "            ticket_analysis = {\n",
        "                \"ticket\": ticket_text,\n",
        "                \"correct\": answers['correct'],\n",
        "                \"prompted\": answers['final_answer'],\n",
        "                \"analysed\": ticket_class\n",
        "                \n",
        "            }\n",
        "            ticket_analysis_results.append(ticket_analysis)\n",
        "            json.dump(ticket_analysis_results, f, indent=2)\n",
        "        number_tries_rate_limit = 0\n",
        "        progress = len(ticket_analysis_results) / len(tickets_sample) * 100\n",
        "        print('\\r\\x1b[2K', f\"\\rProgress: {progress:.2f}%\", end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "divergent_classification = []\n",
        "with open(\"check_generated_tickets.json\", \"w\") as f:\n",
        "    for i in range(len(responses)):\n",
        "        answer = parse_response(tickets_sample[i]['prompt'], responses[i])\n",
        "        if answer['final_answer'] != answer['prompt']:\n",
        "            divergent_classification.append(\n",
        "                {\n",
        "                    \"prompt\": answer['prompt'],\n",
        "                    \"analysis\": answer['final_answer'],\n",
        "                    \"ticket_text\": tickets_sample[i]['final_answer']\n",
        "                }\n",
        "            ) \n",
        "    json.dump(divergent_classification, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAQmQa3-u1x"
      },
      "source": [
        "### Other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDRwq7Gh591e"
      },
      "outputs": [],
      "source": [
        "# from langchain_ollama.chat_models import ChatOllama\n",
        "\n",
        "# # Initialize the ChatOllama model\n",
        "# model_llama = ChatOllama(\n",
        "#     model=model,  # Specify the model version\n",
        "#     base_url=\"http://localhost:11434\",  # URL where Ollama is running locally\n",
        "#     temperature=0.7,  # Control the randomness of the output (0.0 to 1.0)\n",
        "# )\n",
        "\n",
        "# # Note: Ensure Ollama is running on your computer before executing this code\n",
        "\n",
        "# # If you encounter an OllamaEndpointNotFoundError, you may need to pull the model\n",
        "# # Run the following command in your terminal:\n",
        "# # ollama pull llama3.1\n",
        "\n",
        "# # Generate a response from the model\n",
        "# response = model_llama.invoke(\"Olá, meu nome é Yuri. Qual é o seu nome?\")\n",
        "\n",
        "# # Print the response\n",
        "# print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
